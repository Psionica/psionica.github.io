<!DOCTYPE html><head><meta property="og:image" content="/assets/images/dual-thumbnail.png"><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"><link rel="shortcut icon" type="image/icon" href="/favicon.ico"> <script type="text/javascript" src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Dual | Psionica</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Dual" /><meta property="og:locale" content="en_US" /><meta name="description" content="Amplifying knowledge work through user-defined assistants." /><meta property="og:description" content="Amplifying knowledge work through user-defined assistants." /><link rel="canonical" href="http://localhost:4000/tools/dual/" /><meta property="og:url" content="http://localhost:4000/tools/dual/" /><meta property="og:site_name" content="Psionica" /><meta property="og:image" content="http://localhost:4000/assets/images/dual-thumbnail.png" /> <script type="application/ld+json"> {"@type":"WebPage","image":"http://localhost:4000/assets/images/dual-thumbnail.png","url":"http://localhost:4000/tools/dual/","headline":"Dual","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/psionica-transparent-cropped.png"}},"description":"Amplifying knowledge work through user-defined assistants.","@context":"https://schema.org"}</script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="http://localhost:4000/" class="site-title lh-tight"><img src="/assets/images/psionica-transparent-cropped.png" width="30px">&thinsp; Psionica </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a><li class="nav-list-item"><a href="http://localhost:4000/community/" class="nav-list-link">Community</a><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/events/events/" class="nav-list-link">Events</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/events/unconference/" class="nav-list-link">Augment Minds 2021</a></ul><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/tools/tools/" class="nav-list-link">Tools</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/tools/conceptarium/" class="nav-list-link">Conceptarium</a><li class="nav-list-item active"><a href="http://localhost:4000/tools/dual/" class="nav-list-link active">Dual</a><li class="nav-list-item "><a href="http://localhost:4000/tools/autocards/" class="nav-list-link">Autocards</a><li class="nav-list-item "><a href="http://localhost:4000/tools/memnav/" class="nav-list-link">MemNav</a><li class="nav-list-item "><a href="http://localhost:4000/tools/k-probes/" class="nav-list-link">K-Probes</a><li class="nav-list-item "><a href="http://localhost:4000/tools/semantica/" class="nav-list-link">Semantica</a></ul></ul></nav><footer class="site-footer"></div><div class="main" id="top"><div id="main-header" class="main-header"><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="//github.com/psionica" class="site-button" > GitHub </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/tools/tools/">Tools</a><li class="breadcrumb-nav-list-item"><span>Dual</span></ol></nav><div id="main-content" class="main-content" role="main"><h1 class="d-inline-block mt-4 no_toc" id="dual"> <a href="#dual" class="anchor-heading" aria-labelledby="dual"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Dual</h1><p class="label">STAGE 3</p><p class="fs-6 fw-300 text-left mb-0 mt-0">Amplifying knowledge work through user-defined assistants.</p><p class="mt-1">March 2021, by @benjamin, <a href="https://paulbricman.com/">@thoughtware.engineer</a></p><p><a href="https://youtu.be/58-Ue2mDy0k?t=3236" class="btn btn-primary fs-5 mb-4 mb-md-0 mr-2">Watch Demo</a> <a href="https://github.com/Psionica/Dual" class="btn fs-5 mb-4 mb-md-0 mr-2">View Code</a></p><h2 class="no_toc text-delta" id="table-of-contents"> <a href="#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Table of contents</h2><ol id="markdown-toc"><li><a href="#introduction" id="markdown-toc-introduction">Introduction</a><li><a href="#paradigms" id="markdown-toc-paradigms">Paradigms</a><li><a href="#structure" id="markdown-toc-structure">Structure</a><li><a href="#skills" id="markdown-toc-skills">Skills</a><li><a href="#further-steps" id="markdown-toc-further-steps">Further Steps</a><ol><li><a href="#streamlining-installation" id="markdown-toc-streamlining-installation">Streamlining installation</a><li><a href="#documentation" id="markdown-toc-documentation">Documentation</a><li><a href="#standalone-version" id="markdown-toc-standalone-version">Standalone version</a><li><a href="#skills-hub" id="markdown-toc-skills-hub">Skills hub</a></ol><li><a href="#acknowledgements" id="markdown-toc-acknowledgements">Acknowledgements</a><li><a href="#support-us" id="markdown-toc-support-us">Support Us</a><li><a href="#join-us" id="markdown-toc-join-us">Join Us</a><li><a href="#references" id="markdown-toc-references">References</a></ol><hr /><h2 id="introduction"> <a href="#introduction" class="anchor-heading" aria-labelledby="introduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction</h2><p>In his visionary short story titled “Learning to Be Me,” Greg Egan describes the Ndoli Dual, a fictional brain implant which constantly monitors the host’s neural activity in an attempt to learn how it unfolds.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> After several years of collecting data, the Ndoli Dual becomes powerful enough to accurately forecast the neural activity of its host, prompting many to <em>switch</em> to it completely, outsourcing their entire thought process and achieving immortality. Besides describing a thought experiment which puts the Chinese room to shame in terms of vividness, Egan hints at an intriguing possibility – using a proxy for human thought as an optimization target for creating human-like artificial intelligence. An elegant formalism for <em>thinking humanly</em>, as per Russell and Norvig’s taxonomy.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup></p><p class="quote">“If the human race <em>was</em> replacing itself with clockwork automata, I was better off dead; I lacked the blind conviction to join the psychotic underground – who, in any case, were tolerated by the authorities only so long as they remained ineffectual. On the other hand, if all my fears were unfounded – if my sense of identity could survive the switch as easily as it had already survived such traumas as sleeping and waking, the constant death of brain cells, growth, experience, learning and forgetting – then I would gain not only eternal life, but an end to my doubts and my alienation.” – Greg Egan<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup></p><p>However, even if neural activity seems to be the most accurate proxy for thought, accessible high-resolution neuroimaging techniques are a long way from being commercially viable. In order to implement Egan’s envisioned device, we need a proxy for thought which is cheap and abundant, so that our models have plenty of data to learn from. What we need is written language, the next best thing after neural activity in terms of capturing human thought processes. Infinitely expressive and highly economic, written language has been the preferred medium for capturing thoughts for centuries. Today, internet archives contain billions of them, from the most brilliant to the darkest.</p><p class="quote">“You, I hope, are one of those explorers. You, I hope, found these sheets of copper and deciphered the words engraved on their surfaces. And whether or not your brain is impelled by the air that once impelled mine, through the act of reading my words, the patterns that form your thoughts become an imitation of the patterns that once formed mine. And in that way I live again, through you.” – Ted Chiang<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup></p><p>This abundance, naturally, powered recent advances in natural language processing. That said, current language models typically reflect the aggregate thought patterns of millions of people who have contributed data to the training set, whether willingly or not. However, disciplined note-taking practices such as the Zettelkasten and digital gardening enable the fine-tuning of generic language models to one’s specific way of thinking.<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote">4</a></sup> By using hundreds of written notes as training data, we can configure a model to <em>learn to be the note-taker</em>, to internalize their thought patterns, to adopt their writing style and interests, to approximate their mannerisms and responses. After several months of disciplined note-taking, the aligned model becomes powerful enough to accurately extrapolate the written thoughts of its user, enabling transformative new ways of conducting knowledge work and an early glimpse into immortality.</p><p>We’ll now explore the inner workings of Dual, the piece of software which learns to be its user, named so as a tribute to Egan. However, before doing that, we’ll consider some useful framings for better understanding what your Dual is and what it isn’t.</p><hr /><h2 id="paradigms"> <a href="#paradigms" class="anchor-heading" aria-labelledby="paradigms"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Paradigms</h2><h3 class="no_toc" id="your-dual-is-a-skilled-virtual-assistant-for-knowledge-work"> <a href="#your-dual-is-a-skilled-virtual-assistant-for-knowledge-work" class="anchor-heading" aria-labelledby="your-dual-is-a-skilled-virtual-assistant-for-knowledge-work"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Your Dual is a skilled virtual assistant for knowledge work</h3><p>While conversing with your Dual, you can ask it to help you with things. You might want to find notes related to a certain topic, brainstorm research questions, or get your hands on a summary of an article. What your Dual can do for you is entirely determined by its set of skills, also refered to as its skillset. Skills are simply Markdown files which specify the desired behavior of your virtual assistant <em>in natural language</em>.<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote">5</a></sup> Generally, you can teach it new skills by merely describing them in plain English (or in other languages, for that matter). Following this learning phase, your Dual not only adopts your way of thinking, but can also use its skills to best follow your commands before producing responses as chat messages.</p><p class="quote">“The best interface to my brain is a relationship. That’s how merging feels like. That’s how I envision it.” – George Hotz<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote">6</a></sup></p><h3 class="no_toc" id="your-dual-is-your-second-brain-come-to-life"> <a href="#your-dual-is-your-second-brain-come-to-life" class="anchor-heading" aria-labelledby="your-dual-is-your-second-brain-come-to-life"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Your Dual is your second brain come to life</h3><p>The ideas, concepts, and insights which you include in your second brain make up your Dual’s long-term memory. When using skills based on this memory system, relevant pieces of knowledge are strategically remembered behind the scenes in a context-dependent manner. More often than not, those memories are used to provide an explicit context for the previously fine-tuned language model when producing an original response. The end result of using this cognitive architecture is an expressive chatbot which seamlessly integrates disparate fragments of your knowledge with your style of writing.</p><p class="quote">“And if one has to write anyway, it is useful to take advantage of this activity in order to create in the system of notes a competent partner of communication.” – Niklas Luhmann<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">7</a></sup></p><hr /><h2 id="structure"> <a href="#structure" class="anchor-heading" aria-labelledby="structure"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Structure</h2><p>As mentioned before, your Dual has at its core a fine-tuned language model. Currently, it’s using GPT-2, a language model which has been open sourced in late 2019 and can run on average consumer hardware.<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote">8</a></sup> However, in the upcoming months, Dual will advance to GPT-Neo, an open source replica of GPT-3 created by <a href="https://www.eleuther.ai/">EleutherAI</a> which outperforms GPT-3 on many benchmarks.<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote">9</a></sup> We’re very pleased that different open collectives can build on each other’s work so easily thanks to the open source ecosystem. For users who want to opt-out of running the language model on their machine for free, we’ll provide a wrapper around OpenAI’s hosted <a href="https://beta.openai.com/pricing">offering</a>.</p><p>Another essential component of Dual is the skill interpreter. It orchestrates the use of skills at a high-level, and is responsible for the following: determining when to use what skill, keeping track of skills which use other skills, understanding your commands, and so on. It’s conceptually similar to a traditional interpreter of programming languages, such as the Python or Javascript ones, but it’s designed to interpret natural language expressed in plain text and then act on it. In this, Dual is also an important step in developing a new programming paradigm, one based on humane representations such as natural language or 3D scapes, as opposed to clunky symbolic syntax.<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote">10</a></sup> <sup id="fnref:20" role="doc-noteref"><a href="#fn:20" class="footnote">11</a></sup></p><p>Interacting with Dual is done through a familiar chat interface. Dual is reading your messages and then typing out responses using its skills. The chat has full support for Markdown formatting, meaning that if your Dual eventually sends you a list, a table, or even an image, they all get rendered in the chat as responses. While the current implementation is packaged as an <a href="https://obsidian.md/">Obsidian</a> plugin, Dual will integrate with other tools in the future (e.g. <a href="https://roamresearch.com/">Roam Research</a>). Mobile speech interfaces around Dual are also on the table.<sup id="fnref:21" role="doc-noteref"><a href="#fn:21" class="footnote">12</a></sup></p><p>As teaching Dual to recreate itself from scratch is somewhat beyond the capabilities of current language models, Dual itself is implemented using several programming languages. Managing the language model was initially done using <a href="https://huggingface.co/transformers/">Python</a>, but we’re experimenting with <a href="https://github.com/guillaume-be/rust-bert">Rust</a> and <a href="https://www.tensorflow.org/js">Javascript</a> for better cross-platform support. The skill interpreter and the interface are both written in Typescript, playing nicely with an Electron app like Obsidian.</p><hr /><h2 id="skills"> <a href="#skills" class="anchor-heading" aria-labelledby="skills"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Skills</h2><p>This section provides a brief tour of the types of skills your Dual can currently acquire. This isn’t designed to serve as an in-depth tutorial, just as a quick glance at the skills you can teach Dual using Markdown. Skills and examples of using them are provided together in the actual screenshots below, with the skill files on the left and the chat conversations on the right. Keep in mind that only <em>rendered</em> Markdown is shown, for simplicity.</p><p>For starters, let’s say you’re a researcher in academia and want to teach your (virtual) assistant how to come up with research questions on different subjects. To best explain this task, you might choose to provide a few examples of what you consider to be good research questions. This is what happens in the first few lines of the file listed below (have a look!). However, remember that you want to teach it how to come up with suggestions on <em>new</em> subjects. To describe this task, you might use the placeholder <code class="language-plaintext highlighter-rouge">*subject*</code> and ask it to complete the pattern with a new sentence. Placeholders are automatically filled in when the skill is used for following commands.</p><p class="border"><img src="/assets/images/dual_formulate_research_questions.png" alt="" /></p><p>After teaching your Dual this skill (by simply creating the skill file), it will automatically figure out when and how to use it, as can be seen in the chat. Actually, the <code class="language-plaintext highlighter-rouge">Write a paragraph...</code> part is just another command you can issue to your Dual yourself in the chat! It’s skills building on each other all the way down.</p><p>The next example is similar. Provide some examples, a placeholder or two, ask it to complete the pattern, and you just taught it a new skill. You can use any placeholders you wish (e.g. <code class="language-plaintext highlighter-rouge">*person*</code>, <code class="language-plaintext highlighter-rouge">*language*</code>, <code class="language-plaintext highlighter-rouge">*property*</code>) and provide as many of them as you like. Dual knows how to fill them in automatically, provided you actually specify them in your commands.</p><p class="border"><img src="/assets/images/dual_find_related_concepts.png" alt="" /></p><p>However, providing examples might not always be the best way of teaching a skill. For instance, let’s say you’d like your Dual to know how to answer open-ended questions using knowledge from your notes. In only a few lines of text, you can specify that exact behavior without even providing examples. Placeholders get filled in as usual, notes related to the topic are being retrieved, and the pattern gets completed. You can teach it skills which build on any number of other skills.</p><p class="border"><img src="/assets/images/dual_answer_open_questions.png" alt="" /></p><p>The following skill is also learned in a <em>zero-shot</em> fashion, meaning that no examples are provided, as opposed to the first two skills which were acquired in a <em>few-shot</em> way. If you got what those terms refer to, you have a pretty good chance of understanding the main innovation introduced by the GPT-3 paper, which is applied here to other language models.<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote">13</a></sup> Actually, Dual can also be seen as simply a framework for prompt engineering plus some handy features for enabling a conversational interface.</p><p>The skill below is all about prompting the user to reflect on certain topics described in their notes. You might have also noticed <code class="language-plaintext highlighter-rouge">#0</code> being used several times in those examples. That’s indeed probably the least natural part of the otherwise pretty casual “syntax” used here. <code class="language-plaintext highlighter-rouge">#0</code> is simply shorthand for “everything before this block,” while using <code class="language-plaintext highlighter-rouge">#1</code> would refer to the result of the first block, and so on. This design choice was inspired by an exploration of approaches to capability amplification created by <a href="https://ought.org/">Ought</a>, another non-profit operating in this space.<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote">14</a></sup></p><p class="border"><img src="/assets/images/dual_formulate_questions.png" alt="" /></p><p>You can teach your Dual skills using plain natural language. However, if you want to prescribe a more programmatic task, such as computing the result of a mathematical expression or fetching data from an endpoint, you can also throw in some snippets of code by simply using existing Markdown code blocks. Because those blocks are interpreted differently, Markdown files become analogous to <a href="https://jupyter.org/">Jupyter</a> or <a href="https://observablehq.com/@freedmand/sounds">Observable</a> notebooks.</p><p>Back to the skill below. This one relies on some Javascript to get information about a certain entity from <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">Wikidata</a>, a machine-friendly Wikipedia spin-off which advocates for linked open data. Even in the occasional case when you resort to writing code, you’re still free to use placeholders (e.g. <code class="language-plaintext highlighter-rouge">*entity*</code> and <code class="language-plaintext highlighter-rouge">*property*</code> below), making it possible to mix highly deterministic code with fuzzy command parsing. The dawn of versatile user-defined assistants which can integrate with any number of <a href="https://ifttt.com/home">IFTTT</a> services?</p><p class="border"><img src="/assets/images/dual_ask_wikipedia.png" alt="" /></p><p>This section merely provides an overview of how your Dual acquires skills. You could teach it to come up with metaphors, summarize short documents, suggest writing prompts, and what not. All in a local-first, user-defined, and open source way. You can have a look at <a href="https://www.buildgpt3.com/">this awesome list</a> for inspiration on framing all sorts of tasks as text generation, from writing code based on a natural language description to formulating arguments and counter-arguments. More resources on teaching Dual such skills will follow once it will graduate from an alpha version.</p><hr /><h2 id="further-steps"> <a href="#further-steps" class="anchor-heading" aria-labelledby="further-steps"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Further Steps</h2><p>What we’ve made is only the beginning. Here are other ideas to take the project, given further investment of time and other resources.</p><h3 id="streamlining-installation"> <a href="#streamlining-installation" class="anchor-heading" aria-labelledby="streamlining-installation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Streamlining installation</h3><p class="no-toc">Making it simple to get Dual running would help get users onboard, especially in the Obsidian environment. Ideally, a one-click install would make it accessible for non-technical users as well.</p><h3 id="documentation"> <a href="#documentation" class="anchor-heading" aria-labelledby="documentation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Documentation</h3><p class="no-toc">Documenting the Markdown-like language for defining skills and the API for interacting with the server component would make it easy for users to get started and tweak it to their own needs.</p><h3 id="standalone-version"> <a href="#standalone-version" class="anchor-heading" aria-labelledby="standalone-version"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Standalone version</h3><p class="no-toc">Developing Dual for Obsidian is a great first step, with many users gaining a deeper access to their notes. By creating a standalone client, with access from the commandline or other clients, Dual can be a consistent tool for thinking outside of the Obsidian ecosystem.</p><h3 id="skills-hub"> <a href="#skills-hub" class="anchor-heading" aria-labelledby="skills-hub"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Skills hub</h3><p class="no-toc">Users are able to share Dual skills among themselves by essentially transferring the relevant Markdown files. A hub or market for such files would make it easier for users to teach their Duals new skills.</p><hr /><h2 id="acknowledgements"> <a href="#acknowledgements" class="anchor-heading" aria-labelledby="acknowledgements"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Acknowledgements</h2><p>Our work is supported by awesome people: <a href="https://opencollective.com/adam-wiggins">Adam Wiggins</a>, <a href="https://opencollective.com/alex-hubert-iwaniuk">Alex Iwaniuk</a>, <a href="https://stuhlmueller.org/">Andreas Stuhlmüller</a>, <a href="https://opencollective.com/bruno-winck">Bruno Winck</a>, <a href="https://opencollective.com/chris-boette">Chris Boette</a>, <a href="https://opencollective.com/david-d">David Dohan</a>, <a href="https://opencollective.com/flancia">Flancia</a>, <a href="https://opencollective.com/i-do-recall-inc">I Do Recall, Inc.</a>, <a href="https://opencollective.com/nickmilo">Nick Milo</a>, <a href="https://opencollective.com/maggie-appleton">Maggie Appleton</a>, <a href="https://opencollective.com/serjhunt_ark">Serj Hunt</a>, <a href="https://opencollective.com/yangwao">Yang Wao</a>.</p><h2 id="support-us"> <a href="#support-us" class="anchor-heading" aria-labelledby="support-us"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Support Us</h2><p>Our only stakeholder is humanity. Help us keep it that way.</p><p>By supporting our efforts, you’re investing in transparent research and development at the frontier of thought. Not only are you helping us deliver open source tools reshaping the landscape of knowledge work, but you’re also setting in motion a broader movement around cognitive augmentation as a second-order consequence.</p><p><a href="https://opencollective.com/psionica" class="btn btn-primary fs-5 mb-4 mb-md-0">Become a supporter</a></p><hr /><h2 id="join-us"> <a href="#join-us" class="anchor-heading" aria-labelledby="join-us"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Join Us</h2><p>Do you want to contribute to the development of this project yourself? Join Psionica if you want to be part of an enthusiastic community of designers, researchers, and developers committed to empowering individuals around the world in fundamentally new ways. Contribute to existing projects, develop your own, or just hang around for an insightful chat.</p><p><a href="https://discord.gg/NXYZUbhMNf" class="btn btn-primary fs-5 mb-4 mb-md-0">Become a member</a></p><hr /><h2 id="references"> <a href="#references" class="anchor-heading" aria-labelledby="references"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> References</h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>Greg Egan,<br /><a href="http://www.gregegan.net/BIBLIOGRAPHY/Bibliography.html#p12">Learning to Be Me</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p><li id="fn:2" role="doc-endnote"><p>Stuart Russell &amp; Peter Norvig,<br /><a href="http://aima.cs.berkeley.edu/contents.html">AI: A Modern Approach</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:3" role="doc-endnote"><p>Ted Chiang,<br /><a href="https://www.goodreads.com/book/show/41160292-exhalation">Exhalation</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:15" role="doc-endnote"><p>David Clear,<br /><a href="https://writingcooperative.com/zettelkasten-how-one-german-scholar-was-so-freakishly-productive-997e4e0ca125">Zettelkasten</a> <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:16" role="doc-endnote"><p>Matt Cone,<br /><a href="https://www.markdownguide.org/getting-started/">The Markdown Guide</a> <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:6" role="doc-endnote"><p>George Hotz &amp; Lex Fridman,<br /><a href="https://youtu.be/iwcYp-XT7UI?t=6812">Lex Podcast #31</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:5" role="doc-endnote"><p>Niklas Luhmann,<br /><a href="https://luhmann.surge.sh/communicating-with-slip-boxes">Communicating with Slip Boxes</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:17" role="doc-endnote"><p>OpenAI,<br /><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:18" role="doc-endnote"><p>EleutherAI,<br /><a href="https://github.com/EleutherAI/gpt-neo">GPT-Neo</a> <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:19" role="doc-endnote"><p>Bret Victor,<br /><a href="https://vimeo.com/115154289">The Humane Representation of Thought</a> <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:20" role="doc-endnote"><p>Vi Hart &amp; Evelyn Eastmond,<br /><a href="https://youtu.be/FLENZ_si7N8?t=1947">Explorations into Embodied Knowledge and AR/VR</a> <a href="#fnref:20" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:21" role="doc-endnote"><p>Michael Hansen,<br /><a href="https://voice2json.org/">voice2json</a> <a href="#fnref:21" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:13" role="doc-endnote"><p>OpenAI,<br /><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a> <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:14" role="doc-endnote"><p>Ought,<br /><a href="https://ought.org/research/factored-cognition/taxonomy#pointers">A taxonomy of approaches to capability amplification</a> <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div><hr><footer><p><a href="#top" id="back-to-top">Back to top</a></p><p class="text-small text-grey-dk-100 mb-0"></p><p xmlns:cc="http://creativecommons.org/ns#" >Except where otherwise noted, content on this site is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer">CC BY-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p><div class="d-flex mt-2"></div></footer></div></div></div><script async defer src="https://buttons.github.io/buttons.js"></script>
